---
title: Insert title here
key: 85e49b280cb2436cac6fb7334cd914f3

---
## Introduction to PyTorch

```yaml
type: "TitleSlide"
key: "d6bc1904b3"
```

`@lower_third`

name: Reshama Shaikh
title: Data Scientist


`@script`
Welcome to the course.  In this video, you will be learning about PyTorch, a deep learning library.


---
## Popular Deep Learning (DL) Libraries

```yaml
type: "FullSlide"
key: "656a11b3ca"
center_content: false
```

`@part1`
![libraries](https://assets.datacamp.com/production/repositories/3590/datasets/6f43dbbc600cfdf3f08186bc9da76d6a34b6278c/dl_libraries.png)


`@script`
There are a number of deep learning libraries available:  Here’s a list of some of the most popular deep learning libraries, and the organizations who developed and support them.


---
## Popular High-Level Deep Learning Libraries

```yaml
type: "FullSlide"
key: "7c2baad707"
```

`@part1`
![highlevel](https://assets.datacamp.com/production/repositories/3590/datasets/fd28f0867ea87b564ceec8cc4c1c6b1d021f9309/dc_highlevel.png)


`@script`
There are also some popular “high-level” deep learning libraries, which are built on top of, or “wrapped around” a deep learning library to make it easier to use.  It helps users get to deep learning faster.

Keras is one of those “high level” libraries, and it runs on top of a number of DL libraries such as TensorFlow, Theano, Cognitive Toolkit and MxNet.  

Fastai is a DL library that used to run on top of Keras.


---
## Popular High-Level Deep Learning Libraries

```yaml
type: "FullSlide"
key: "fd95d7bc6d"
```

`@part1`
![highlevel](https://assets.datacamp.com/production/repositories/3590/datasets/ad5cd6fb3a9f94c9c8f1b88c3c4b6ca5c1e575c5/dc_fastai.png)


`@script`
But, then PyTorch was released, and it was much easier to use, given that it was written in Python.  Now Fastai is built on top of PyTorch.


---
## High-Level Deep Learning Libraries

```yaml
type: "FullSlide"
key: "eb3180ff8f"
```

`@part1`
![](https://assets.datacamp.com/production/repositories/3590/datasets/585732930cdb765e60a79f22b5a1a3b989626b56/dc_pytorch1.png)


`@script`
In this course, we will focus on PyTorch.


---
## History of PyTorch Development

```yaml
type: "FullSlide"
key: "227511aaec"
```

`@part1`
![](https://assets.datacamp.com/production/repositories/3590/datasets/7020f84bc7b92f818ef2bd636efb8755e20b8c76/dc_pytorch_hx.png)


`@script`
Let’s look at the history of PyTorch.  Python was first released in 1990.  Lua was developed in 1993, in Brazil and written in C.  

In 2002, Torch an ML / DL package was released, and it was built on top of Lua.  Torch was used for a number of years.  However, there was no ecosystem for Lua, no standard library.  Python seemed to be the best ecosystem, and a new version of Torch was developed in Python.  

It was called PyTorch and its initial release was in January 2017.   What makes PyTorch so versatile and powerful is that it is able to tap into all the functionality of Python. Because Python is used widely, its syntax carries over into PyTorch, which makes the coding more familiar.  Hence, it is faster to learn and use.


---
## Numpy

```yaml
type: "FullSlide"
key: "18d0ad23ca"
```

`@part1`
Let's create some arrays using Numpy:
```
import numpy as np

# create an empty 2-d Numpy array
np.random.seed(0)
x = np.empty([5,3])
```
{{1}}
Let's get some information on the array:  {{2}}
```python
print(type(x))
print(len(x))
print(x.shape)
print(x)
```
{{3}}


`@script`



---
## PyTorch

```yaml
type: "FullSlide"
key: "e2d184c94c"
```

`@part1`
Tensors are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing.  Let's create some Torch tensors.
```python
import torch
```
{{1}}
```python
# construct a 5x3 matrix, uninitialized
x = torch.empty(5, 3)
```
{{2}}
This is the code we use to see what type our tensors  {{3}}
```python
print(type(x))
print(len(x))
print(x.shape)
print(x)
```
{{4}}


`@script`



---
## PyTorch:  Tensors

```yaml
type: "FullSlide"
key: "e65c3bae5c"
```

`@part1`
Let's look at the output {{1}}
```bash
<class 'torch.Tensor'>
5
torch.Size([5, 3])
tensor([[0.4963, 0.7682, 0.0885],
        [0.1320, 0.3074, 0.6341],
        [0.4901, 0.8964, 0.4556],
        [0.6323, 0.3489, 0.4017],
        [0.0223, 0.1689, 0.2939]])
```
{{2}}


`@script`



---
## Numpy:  output

```yaml
type: "FullSlide"
key: "c415fe4823"
```

`@part1`
And here is the output:  {{1}}
```bash
<class 'numpy.ndarray'>
5
(5, 3)
[[ 8.39911598e-323  0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  6.95312350e-310 -2.31584178e+077]]
```
{{2}}


`@script`



---
## Let's practice

```yaml
type: "FinalSlide"
key: "e84b671970"
```

`@script`


