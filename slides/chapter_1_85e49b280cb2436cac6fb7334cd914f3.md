---
title: Insert title here
key: 85e49b280cb2436cac6fb7334cd914f3

---
## Introduction to PyTorch

```yaml
type: "TitleSlide"
key: "d6bc1904b3"
```

`@lower_third`

name: Reshama Shaikh
title: Data Scientist


`@script`
Welcome to the course.  In this video, you will be learning about PyTorch, a deep learning library.


---
## Popular Deep Learning (DL) Libraries

```yaml
type: "FullSlide"
key: "656a11b3ca"
center_content: false
```

`@part1`
![libraries](https://assets.datacamp.com/production/repositories/3590/datasets/6f43dbbc600cfdf3f08186bc9da76d6a34b6278c/dl_libraries.png)


`@script`
There are a number of deep learning libraries available:  Here’s a list of some of the most popular deep learning libraries, and the organizations who developed and support them.


---
## Popular "High-Level" Deep Learning Libraries

```yaml
type: "FullSlide"
key: "7c2baad707"
```

`@part1`
![highlevel](https://assets.datacamp.com/production/repositories/3590/datasets/fd28f0867ea87b564ceec8cc4c1c6b1d021f9309/dc_highlevel.png)


`@script`
There are also some popular “high-level” deep learning libraries, which are built on top of, or “wrapped around” a deep learning library to make it easier to use.  It helps users get to deep learning faster.

Keras is one of those “high level” libraries, and it runs on top of a number of DL libraries such as TensorFlow, Theano, Cognitive Toolkit and MxNet.  

Fastai is a DL library that _used to_ run on top of Keras.


---
## Popular "High-Level" Deep Learning Libraries

```yaml
type: "FullSlide"
key: "fd95d7bc6d"
```

`@part1`
![highlevel](https://assets.datacamp.com/production/repositories/3590/datasets/ad5cd6fb3a9f94c9c8f1b88c3c4b6ca5c1e575c5/dc_fastai.png)


`@script`
But, then PyTorch was released, and it was much easier to use, given that it was written in Python.  Now Fastai is built on top of PyTorch.


---
## High-Level Deep Learning Libraries

```yaml
type: "FullSlide"
key: "eb3180ff8f"
disable_transition: true
```

`@part1`
![](https://assets.datacamp.com/production/repositories/3590/datasets/585732930cdb765e60a79f22b5a1a3b989626b56/dc_pytorch1.png)


`@script`
In this course, we will focus on PyTorch.


---
## History of PyTorch Development

```yaml
type: "FullSlide"
key: "227511aaec"
```

`@part1`
![](https://assets.datacamp.com/production/repositories/3590/datasets/7020f84bc7b92f818ef2bd636efb8755e20b8c76/dc_pytorch_hx.png)


`@script`
Let’s look at the history of PyTorch.  

Python was first released in **1990.**  This programming language has a large standard library.  The Python Package Index, called PyPI, is the official repository for 3rd party Python software.  It contains over 130K packages which have a wide range of functionality.  

Lua was developed in 1993, in Brazil and written in C.  

In 2002, Torch, a Machine Learning / Deep Learning package, was released, and it was built on top of Lua.  Torch was used for a number of years.  However, there was no ecosystem for Lua, no standard library.  Python seemed to be the best ecosystem, and a new version of Torch was developed in Python.  

It was called PyTorch and its initial release was in January 2017.   

What makes PyTorch so versatile and powerful is that it is able to tap into all of the functionality of Python. Because Python is used widely, its syntax carries over into PyTorch, which makes the coding more familiar.  Hence, it is faster to learn and use.


---
## PyTorch Features

```yaml
type: "FullSlide"
key: "21bf2c017d"
```

`@part1`
- has an Ndarray library with GPU support, it is a Numpy alternative
- has an **automatic differentiation package** for computation
  - deep learning
  - reinforcement learning
- gradient based optimization package
  - in implements a lot of standard gradient descent optimization methods
- has utility packages
  - data loading
  - automatically downloading pre-trained models from the internet, etc


`@script`



---
## Ndarray Library

```yaml
type: "FullSlide"
key: "9dfdd9e25c"
```

`@part1`
- just like numpy provides an ndarray object, PyTorch has a `torch.Tensor`
  - there are different tensor types: float, double tensor, long tensor
  - `torch.Tensor` is the default type, which is **float tensor**
- 200+ operations:  linear algebra, indexing, ...
- very fast acceleration on NVIDIA GPUs


`@script`



---
## Seamless GPU Tensors

```yaml
type: "FullSlide"
key: "3c7e9df70d"
```

`@part1`
- doing CPU to GPU transfers is fairly expensive
- you wouldn't want to have an automatic way of figuring out where to allocate memory
- doing it explicitly gives the user a lot of power

```python
if torch.cuda.is_available():
    x = x.cuda()
    y = y.cuda()
    x + y
```


`@script`



---
## Power of PyTorch

```yaml
type: "FullSlide"
key: "a3a9d689c4"
```

`@part1`
- if there is a library missing on the CPU side, you can always convert the tensor to a **numpy array** and call scipy operations


`@script`



---
## Numpy

```yaml
type: "FullSlide"
key: "18d0ad23ca"
```

`@part1`
Let's create some arrays using Numpy:
```
import numpy as np

# create an empty 2-d Numpy array
np.random.seed(0)
x = np.empty([5,3])
```
{{1}}
Let's get some information on the array:  {{2}}
```python
print(type(x))
print(len(x))
print(x.shape)
print(x)
```
{{3}}


`@script`



---
## Numpy:  output

```yaml
type: "FullSlide"
key: "c415fe4823"
```

`@part1`
And here is the output:  {{1}}
```bash
<class 'numpy.ndarray'>
5
(5, 3)
[[ 8.39911598e-323  0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  0.00000000e+000  0.00000000e+000]
 [ 0.00000000e+000  6.95312350e-310 -2.31584178e+077]]
```
{{2}}


`@script`



---
## PyTorch

```yaml
type: "FullSlide"
key: "e2d184c94c"
```

`@part1`
Tensors are similar to NumPy’s ndarrays, with the addition being that **Tensors can also be used on a GPU to accelerate computing.**  Let's create some Torch tensors.
```python
import torch
```
{{1}}
```python
# construct a 5x3 matrix, uninitialized
x = torch.empty(5, 3)
```
{{2}}
This is the code we use to see what type our tensors  {{3}}
```python
print(type(x))
print(len(x))
print(x.shape)
print(x)
```
{{4}}


`@script`



---
## PyTorch Tensors:  output

```yaml
type: "FullSlide"
key: "e65c3bae5c"
```

`@part1`
Let's look at the output {{1}}
```bash
<class 'torch.Tensor'>
5
torch.Size([5, 3])
tensor([[0.4963, 0.7682, 0.0885],
        [0.1320, 0.3074, 0.6341],
        [0.4901, 0.8964, 0.4556],
        [0.6323, 0.3489, 0.4017],
        [0.0223, 0.1689, 0.2939]])
```
{{2}}


`@script`



---
## Let's practice

```yaml
type: "FinalSlide"
key: "e84b671970"
```

`@script`


